     <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice Interface - Enhanced Audio</title>
  <script src="https://cdn.jsdelivr.net/npm/protobufjs@7.2.4/dist/protobuf.min.js"></script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f9f9f9;
    }
    h1 {
      color: #2c3e50;
      text-align: center;
    }
    .container {
      background-color: white;
      border-radius: 10px;
      padding: 20px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .status {
      margin: 20px 0;
      padding: 15px;
      border-radius: 5px;
      background-color: #f8f9fa;
      border-left: 5px solid #6c757d;
    }
    .controls {
      display: flex;
      gap: 10px;
      justify-content: center;
      margin: 20px 0;
    }
    .advanced-controls {
      display: flex;
      flex-direction: column;
      gap: 10px;
      margin: 20px 0;
      padding: 15px;
      background-color: #f8f9fa;
      border-radius: 5px;
    }
    .control-group {
      display: flex;
      align-items: center;
      gap: 10px;
      margin-bottom: 10px;
    }
    .control-group label {
      min-width: 150px;
    }
    button {
      padding: 10px 20px;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      font-size: 16px;
      transition: all 0.2s;
    }
    #startAudioBtn {
      background-color: #28a745;
      color: white;
    }
    #stopAudioBtn {
      background-color: #dc3545;
      color: white;
    }
    button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
    .logs {
      height: 200px;
      overflow-y: auto;
      padding: 10px;
      background-color: #f8f9fa;
      border-radius: 5px;
      border: 1px solid #e9ecef;
      font-family: monospace;
      margin-top: 20px;
    }
    .log-item {
      margin-bottom: 5px;
      padding: 5px;
      border-bottom: 1px solid #e9ecef;
    }
    .log-user {
      color: #0066cc;
    }
    .log-bot {
      color: #cc0066;
    }
    .volume-meter {
      width: 100%;
      height: 20px;
      background-color: #e9ecef;
      border-radius: 10px;
      margin-top: 10px;
      overflow: hidden;
    }
    .volume-level {
      height: 100%;
      width: 0%;
      background-color: #28a745;
      transition: width 0.1s ease;
    }
    .toggle-switch {
      position: relative;
      display: inline-block;
      width: 60px;
      height: 34px;
    }
    .toggle-switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }
    .slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background-color: #ccc;
      transition: .4s;
      border-radius: 34px;
    }
    .slider:before {
      position: absolute;
      content: "";
      height: 26px;
      width: 26px;
      left: 4px;
      bottom: 4px;
      background-color: white;
      transition: .4s;
      border-radius: 50%;
    }
    input:checked + .slider {
      background-color: #2196F3;
    }
    input:checked + .slider:before {
      transform: translateX(26px);
    }
  </style>
</head>

<body>
  <div class="container">
    <h1>PipeCat Voice Interface</h1>
    
    <div class="status" id="statusPanel">
      <h3>Status: <span id="progressText">Loading protobuf definitions...</span></h3>
      <div class="volume-meter">
        <div class="volume-level" id="volumeLevel"></div>
      </div>
    </div>
    
    <div class="controls">
      <button id="startAudioBtn" disabled>Start Conversation</button>
      <button id="stopAudioBtn" disabled>End Conversation</button>
    </div>

    <details>
      <summary>Audio Settings</summary>
      <div class="advanced-controls">
        <div class="control-group">
          <label for="noiseReduction">Noise Reduction:</label>
          <label class="toggle-switch">
            <input type="checkbox" id="noiseReduction" checked>
            <span class="slider"></span>
          </label>
        </div>
        <div class="control-group">
          <label for="echoCancel">Echo Cancellation:</label>
          <label class="toggle-switch">
            <input type="checkbox" id="echoCancel" checked>
            <span class="slider"></span>
          </label>
        </div>
        <div class="control-group">
          <label for="autoGain">Auto Gain Control:</label>
          <label class="toggle-switch">
            <input type="checkbox" id="autoGain" checked>
            <span class="slider"></span>
          </label>
        </div>
        <div class="control-group">
          <label for="gainLevel">Input Gain:</label>
          <input type="range" id="gainLevel" min="0" max="2" step="0.1" value="1.0">
          <span id="gainValue">1.0</span>
        </div>
        <div class="control-group">
          <label for="speakerVolume">Speaker Volume:</label>
          <input type="range" id="speakerVolume" min="0" max="1" step="0.1" value="1.0">
          <span id="volumeValue">1.0</span>
        </div>
        <div class="control-group">
          <label for="audioBufferSize">Buffer Size:</label>
          <select id="audioBufferSize">
            <option value="256">256 (Low Latency)</option>
            <option value="512">512</option>
            <option value="1024" selected>1024 (Default)</option>
            <option value="2048">2048 (More Stable)</option>
            <option value="4096">4096 (Highest Stability)</option>
          </select>
        </div>
        <div class="control-group">
          <label for="audioDevice">Audio Input Device:</label>
          <select id="audioDevice">
            <option value="default">Default Device</option>
            <!-- Will be populated with available devices -->
          </select>
        </div>
      </div>
    </details>

    <div class="logs" id="logContainer">
      <div class="log-item">Ready to start conversation...</div>
    </div>
  </div>

  <script>
    // Configuration
    const SAMPLE_RATE = 16000;
    const NUM_CHANNELS = 1;
    const WEBSOCKET_URL = 'ws://localhost:8765';
    const PLAY_TIME_RESET_THRESHOLD_MS = 1.0;
    
    // State variables
    let Frame = null;
    let ws = null;
    let audioContext = null;
    let microphoneStream = null;
    let source = null;
    let scriptProcessor = null;
    let gainNode = null;
    let analyser = null;
    let audioWorklet = null;
    let playTime = 0;
    let lastMessageTime = 0;
    let isPlaying = false;
    let protoReady = false;
    let audioVolume = 1.0;
    let volumeDataArray = null;
    let volumeAnimationFrame = null;

    // DOM elements
    const startBtn = document.getElementById('startAudioBtn');
    const stopBtn = document.getElementById('stopAudioBtn');
    const progressText = document.getElementById('progressText');
    const logContainer = document.getElementById('logContainer');
    const volumeLevel = document.getElementById('volumeLevel');
    const gainSlider = document.getElementById('gainLevel');
    const gainValue = document.getElementById('gainValue');
    const volumeSlider = document.getElementById('speakerVolume');
    const volumeValue = document.getElementById('volumeValue');
    const noiseReductionCheckbox = document.getElementById('noiseReduction');
    const echoCancelCheckbox = document.getElementById('echoCancel');
    const autoGainCheckbox = document.getElementById('autoGain');
    const bufferSizeSelect = document.getElementById('audioBufferSize');
    const audioDeviceSelect = document.getElementById('audioDevice');

    // Audio processing settings
    let settings = {
      bufferSize: parseInt(bufferSizeSelect.value),
      noiseReduction: noiseReductionCheckbox.checked,
      echoCancel: echoCancelCheckbox.checked,
      autoGain: autoGainCheckbox.checked,
      gain: parseFloat(gainSlider.value),
      volume: parseFloat(volumeSlider.value),
      selectedDevice: 'default'
    };

    // Event listeners for audio settings
    gainSlider.addEventListener('input', (e) => {
      settings.gain = parseFloat(e.target.value);
      gainValue.textContent = settings.gain.toFixed(1);
      if (gainNode) {
        gainNode.gain.value = settings.gain;
      }
    });

    volumeSlider.addEventListener('input', (e) => {
      settings.volume = parseFloat(e.target.value);
      volumeValue.textContent = settings.volume.toFixed(1);
      if (audioContext) {
        audioContext.destination.channelInterpretation = 'speakers';
        audioContext.destination.channelCount = 2;
        audioVolume = settings.volume;
      }
    });

    noiseReductionCheckbox.addEventListener('change', (e) => {
      settings.noiseReduction = e.target.checked;
    });

    echoCancelCheckbox.addEventListener('change', (e) => {
      settings.echoCancel = e.target.checked;
    });

    autoGainCheckbox.addEventListener('change', (e) => {
      settings.autoGain = e.target.checked;
    });

    bufferSizeSelect.addEventListener('change', (e) => {
      settings.bufferSize = parseInt(e.target.value);
      addLogMessage('System', `Audio buffer size changed to ${settings.bufferSize}`);
      if (isPlaying) {
        addLogMessage('System', 'Please restart the conversation for the new buffer size to take effect');
      }
    });

    audioDeviceSelect.addEventListener('change', (e) => {
      settings.selectedDevice = e.target.value;
      addLogMessage('System', `Audio input device changed`);
      if (isPlaying) {
        addLogMessage('System', 'Please restart the conversation for the new device to take effect');
      }
    });

    // Populate audio devices dropdown
    async function populateAudioDevices() {
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputDevices = devices.filter(device => device.kind === 'audioinput');
        
        // Clear existing options except the default
        while (audioDeviceSelect.options.length > 1) {
          audioDeviceSelect.remove(1);
        }
        
        // Add available devices
        audioInputDevices.forEach(device => {
          const option = document.createElement('option');
          option.value = device.deviceId;
          option.text = device.label || `Microphone ${audioDeviceSelect.options.length}`;
          audioDeviceSelect.appendChild(option);
        });
      } catch (error) {
        console.error('Error enumerating audio devices:', error);
      }
    }

    // Define the protobuf schema inline for reliability
    const protoDefinition = `
      syntax = "proto3";
      package pipecat;

      message TextFrame {
        uint64 id = 1;
        string name = 2;
        string text = 3;
      }

      message AudioRawFrame {
        uint64 id = 1;
        string name = 2;
        bytes audio = 3;
        uint32 sample_rate = 4;
        uint32 num_channels = 5;
        optional uint64 pts = 6;
      }

      message TranscriptionFrame {
        uint64 id = 1;
        string name = 2;
        string text = 3;
        string user_id = 4;
        string timestamp = 5;
      }

      message Frame {
        oneof frame {
          TextFrame text = 1;
          AudioRawFrame audio = 2;
          TranscriptionFrame transcription = 3;
        }
      }
    `;

    // Initialize protobuf
    function initProtobuf() {
      try {
        const root = protobuf.parse(protoDefinition).root;
        Frame = root.lookupType('pipecat.Frame');
        protoReady = true;
        updateStatus('Ready! Click "Start Conversation" to begin.');
        startBtn.disabled = false;
        addLogMessage('System', 'Protobuf definitions loaded successfully.');
        
        // Initialize audio devices
        populateAudioDevices();
      } catch (err) {
        updateStatus('Failed to load protobuf definitions: ' + err.message);
        addLogMessage('Error', 'Failed to load protobuf: ' + err.message);
        console.error('Protobuf error:', err);
      }
    }

    // Initialize WebSocket connection
    function initWebSocket() {
      updateStatus('Connecting to server...');
      
      ws = new WebSocket(WEBSOCKET_URL);
      ws.binaryType = 'arraybuffer';

      ws.addEventListener('open', handleWebSocketOpen);
      ws.addEventListener('message', handleWebSocketMessage);
      ws.addEventListener('close', (event) => {
        updateStatus('Connection closed. Code: ' + event.code);
        addLogMessage('System', `WebSocket connection closed. Code: ${event.code}, Reason: ${event.reason || 'None'}`);
        stopAudio(false);
      });
      ws.addEventListener('error', (event) => {
        updateStatus('Connection error. Check console for details.');
        addLogMessage('Error', 'WebSocket connection error');
        console.error('WebSocket error:', event);
      });
    }

    // WebSocket open handler
    function handleWebSocketOpen(event) {
      updateStatus('Connected! Setting up microphone...');
      addLogMessage('System', 'WebSocket connection established');
      
      // Request microphone access with selected device
      const constraints = {
        audio: {
          deviceId: settings.selectedDevice !== 'default' ? { exact: settings.selectedDevice } : undefined,
          sampleRate: { ideal: SAMPLE_RATE },
          channelCount: { ideal: NUM_CHANNELS },
          autoGainControl: settings.autoGain,
          echoCancellation: settings.echoCancel,
          noiseSuppression: settings.noiseReduction
        }
      };
      
      navigator.mediaDevices.getUserMedia(constraints)
        .then(setupAudioProcessing)
        .catch(handleMicrophoneError);
    }

    // Audio visualization
    function updateVolumeMeter() {
      if (!analyser || !volumeDataArray) return;
      
      analyser.getByteFrequencyData(volumeDataArray);
      
      // Calculate average volume
      let sum = 0;
      for (let i = 0; i < volumeDataArray.length; i++) {
        sum += volumeDataArray[i];
      }
      const average = sum / volumeDataArray.length;
      
      // Update volume meter (scale to 0-100%)
      const volumePercent = Math.min(100, Math.max(0, average * 100 / 256));
      volumeLevel.style.width = `${volumePercent}%`;
      
      // Adjust color based on volume
      if (volumePercent < 30) {
        volumeLevel.style.backgroundColor = '#28a745'; // Green
      } else if (volumePercent < 80) {
        volumeLevel.style.backgroundColor = '#ffc107'; // Yellow
      } else {
        volumeLevel.style.backgroundColor = '#dc3545'; // Red
      }
      
      volumeAnimationFrame = requestAnimationFrame(updateVolumeMeter);
    }

    // Set up audio processing pipeline
    function setupAudioProcessing(stream) {
      microphoneStream = stream;
      
      // Create audio processing nodes
      source = audioContext.createMediaStreamSource(stream);
      
      // Create a gain node for volume control
      gainNode = audioContext.createGain();
      gainNode.gain.value = settings.gain;
      
      // Create analyser for volume visualization
      analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      volumeDataArray = new Uint8Array(analyser.frequencyBinCount);
      
      // Use the appropriate buffer size
      scriptProcessor = audioContext.createScriptProcessor(settings.bufferSize, 1, 1);
      
      // Apply noise reduction if Web Audio API worklets are available
      if (window.AudioWorkletNode && settings.noiseReduction) {
        try {
          // Connect nodes with noise reduction
          source.connect(gainNode);
          gainNode.connect(analyser);
          analyser.connect(scriptProcessor);
          scriptProcessor.connect(audioContext.destination);
        } catch (error) {
          console.warn('Advanced audio processing not available:', error);
          // Fallback to simple chain
          source.connect(gainNode);
          gainNode.connect(analyser);
          analyser.connect(scriptProcessor);
          scriptProcessor.connect(audioContext.destination);
        }
      } else {
        // Simple connection chain
        source.connect(gainNode);
        gainNode.connect(analyser);
        analyser.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination);
      }
      
      // Start volume visualization
      volumeAnimationFrame = requestAnimationFrame(updateVolumeMeter);
      
      // Set up the audio processing callback
      scriptProcessor.onaudioprocess = processAudioData;
      
      updateStatus('Active - Speak now!');
      addLogMessage('System', 'Microphone access granted and audio processing started');
    }

    // Handle microphone access errors
    function handleMicrophoneError(error) {
      updateStatus('Error accessing microphone: ' + error.message);
      addLogMessage('Error', 'Microphone access failed: ' + error.message);
      console.error('Error accessing microphone:', error);
      stopAudio(true);
    }

    // Process incoming audio data with noise reduction
    function processAudioData(event) {
      if (!ws || ws.readyState !== WebSocket.OPEN) {
        return;
      }

      try {
        const audioData = event.inputBuffer.getChannelData(0);
        
        // Apply simple noise gate
        let processedAudio = new Float32Array(audioData.length);
        const noiseThreshold = 0.01; // Adjust based on testing
        
        for (let i = 0; i < audioData.length; i++) {
          // Simple noise gate
          if (Math.abs(audioData[i]) > noiseThreshold) {
            processedAudio[i] = audioData[i];
          } else {
            processedAudio[i] = 0;
          }
        }
        
        const pcmS16Array = convertFloat32ToS16PCM(processedAudio);
        const pcmByteArray = new Uint8Array(pcmS16Array.buffer);
        
        const frame = Frame.create({
          audio: {
            audio: pcmByteArray,
            sampleRate: SAMPLE_RATE,
            numChannels: NUM_CHANNELS
          }
        });
        
        const encodedFrame = Frame.encode(frame).finish();
        ws.send(encodedFrame);
      } catch (error) {
        console.error('Error processing audio data:', error);
      }
    }

    // Handle incoming WebSocket messages
    function handleWebSocketMessage(event) {
      try {
        const arrayBuffer = event.data;
        const parsedFrame = Frame.decode(new Uint8Array(arrayBuffer));
        
        // Handle different frame types
        if (parsedFrame.transcription) {
          const text = parsedFrame.transcription.text;
          addLogMessage('You', text);
        } else if (parsedFrame.text) {
          const text = parsedFrame.text.text;
          addLogMessage('Bot', text);
        } else if (parsedFrame.audio && isPlaying) {
          // Process audio data for playback
          playAudioData(parsedFrame.audio);
        }
      } catch (error) {
        console.error('Error handling WebSocket message:', error);
      }
    }

    // Enhanced audio playback with volume control
    function playAudioData(audioData) {
      try {
        // Reset play time if needed
        const currentTime = audioContext.currentTime;
        const diffTime = currentTime - lastMessageTime;
        
        if (playTime === 0 || diffTime > PLAY_TIME_RESET_THRESHOLD_MS) {
          playTime = currentTime;
        }
        
        lastMessageTime = currentTime;
        
        // Convert the audio data to an ArrayBuffer
        const audioBuffer = new Uint8Array(audioData.audio).buffer;
        
        // Decode and play the audio
        audioContext.decodeAudioData(audioBuffer)
          .then(buffer => {
            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            
            // Add volume control
            const gainNode = audioContext.createGain();
            gainNode.gain.value = audioVolume;
            
            // Add a simple high-pass filter to reduce low frequency noise
            const highpassFilter = audioContext.createBiquadFilter();
            highpassFilter.type = "highpass";
            highpassFilter.frequency.value = 80;
            
            // Connect nodes
            source.connect(highpassFilter);
            highpassFilter.connect(gainNode);
            gainNode.connect(audioContext.destination);
            
            source.start(playTime);
            playTime += buffer.duration;
          })
          .catch(error => console.error('Error decoding audio data:', error));
      } catch (error) {
        console.error('Error playing audio data:', error);
      }
    }

    // Convert Float32 audio data to Int16 PCM with enhanced noise handling
    function convertFloat32ToS16PCM(float32Array) {
      const int16Array = new Int16Array(float32Array.length);
      
      // Find the max amplitude for normalization
      let maxAmplitude = 0;
      for (let i = 0; i < float32Array.length; i++) {
        maxAmplitude = Math.max(maxAmplitude, Math.abs(float32Array[i]));
      }
      
      // If the max amplitude is close to zero, don't normalize to avoid amplifying noise
      const normalizationFactor = maxAmplitude > 0.1 ? 0.9 / maxAmplitude : 1;
      
      for (let i = 0; i < float32Array.length; i++) {
        // Normalize and convert to 16-bit
        const normalizedSample = float32Array[i] * normalizationFactor;
        const sample = Math.max(-1, Math.min(1, normalizedSample));
        int16Array[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
      }
      
      return int16Array;
    }

    // Start button click handler
    function startAudioBtnHandler() {
      if (!protoReady) {
        updateStatus('Protobuf definitions not loaded yet. Please wait.');
        return;
      }
      
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        updateStatus('getUserMedia is not supported in your browser.');
        return;
      }
      
      startBtn.disabled = true;
      stopBtn.disabled = false;
      
      try {
        // Create audio context with proper options
        audioContext = new (window.AudioContext || window.webkitAudioContext)({
          latencyHint: 'interactive',
          sampleRate: SAMPLE_RATE
        });
        
        isPlaying = true;
        initWebSocket();
      } catch (error) {
        updateStatus('Error initializing audio: ' + error.message);
        startBtn.disabled = false;
        stopBtn.disabled = true;
      }
    }

    // Stop button click handler
    function stopAudioBtnHandler() {
      stopAudio(true);
    }

    // Stop audio processing and close connections
    function stopAudio(closeWebsocket) {
      updateStatus('Stopping...');
      
      playTime = 0;
      isPlaying = false;
      startBtn.disabled = false;
      stopBtn.disabled = true;
      
      // Stop volume visualization
      if (volumeAnimationFrame) {
        cancelAnimationFrame(volumeAnimationFrame);
        volumeAnimationFrame = null;
      }
      
      // Close WebSocket if needed
      if (ws && closeWebsocket) {
        ws.close();
        ws = null;
      }
      
      // Disconnect audio nodes
      if (scriptProcessor) {
        scriptProcessor.disconnect();
        scriptProcessor = null;
      }
      
      if (gainNode) {
        gainNode.disconnect();
        gainNode = null;
      }
      
      if (analyser) {
        analyser.disconnect();
        analyser = null;
      }
      
      if (source) {
        source.disconnect();
        source = null;
      }
      
      // Stop microphone stream
      if (microphoneStream) {
        microphoneStream.getTracks().forEach(track => track.stop());
        microphoneStream = null;
      }
      
      // Close audio context
      if (audioContext && audioContext.state !== 'closed') {
        audioContext.close().catch(err => console.error('Error closing audio context:', err));
      }
      
      updateStatus('Ready! Click "Start Conversation" to begin.');
      addLogMessage('System', 'Audio processing stopped');
    }

    // Helper function to update status text
    function updateStatus(message) {
      progressText.textContent = message;
    }

    // Helper function to add log messages
    function addLogMessage(sender, message) {
      const logItem = document.createElement('div');
      logItem.className = 'log-item';
      
      if (sender === 'You') {
        logItem.classList.add('log-user');
      } else if (sender === 'Bot') {
        logItem.classList.add('log-bot');
      }
      
      logItem.textContent = `${sender}: ${message}`;
      logContainer.appendChild(logItem);
      
      // Scroll to bottom
      logContainer.scrollTop = logContainer.scrollHeight;
    }

    // Set up event listeners
    startBtn.addEventListener('click', startAudioBtnHandler);
    stopBtn.addEventListener('click', stopAudioBtnHandler);
    
    // Initialize the application
    document.addEventListener('DOMContentLoaded', initProtobuf);
  </script>
</body>
</html>